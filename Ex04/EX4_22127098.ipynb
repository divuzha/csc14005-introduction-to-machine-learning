{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "Đinh Vũ Gia Hân - 22127098\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \n",
    "This answer is referenced from [1].\n",
    "\n",
    "With probability $\\geq 1 - \\delta$, the VC generalization bound is given by:\n",
    "$$|E_{out} - E_{in}| \\leq \\sqrt{\\frac{8}{N} \\ln \\left( \\frac{4 m_{\\mathcal{H}}(2N)}{\\delta} \\right) }$$\n",
    "\n",
    "If we want 95% confidence that our generalization error is at most 0.05 which means:\n",
    "$$\\delta = 1 - 0.95 = 0.05$$\n",
    "\n",
    "Furthermore all the answers have $N > d_{VC}$ so we have the approximate bound:\n",
    "\\begin{align}\n",
    "m_{\\mathcal{H}}(N) &= N^{d_{VC}} \\\\\n",
    "\\rightarrow m_{\\mathcal{H}}(2N) &= (2N)^{d_{VC}}\n",
    "\\end{align}\n",
    "\n",
    "So, the bound becomes:\n",
    "$$\\epsilon \\leq \\sqrt{\\frac{8}{N} \\ln \\left( \\frac{4 (2N)^{d_{VC}}}{\\delta} \\right) }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_vc_bound(N, dVC, delta, mH):\n",
    "    \"\"\"\n",
    "    Calculate the original VC bound\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    N : int\n",
    "        Number of samples\n",
    "    dVC : int\n",
    "        VC dimension\n",
    "    delta : float\n",
    "        Confidence level\n",
    "    mH : int\n",
    "        The growth function\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    float\n",
    "        The original VC bound\n",
    "    \"\"\"\n",
    "    return math.sqrt(8 * math.log(4 * mH / delta) / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With N = 400000, omega = 0.05297276596538537\n",
      "With N = 420000, omega = 0.05178593269970576\n",
      "With N = 440000, omega = 0.050678810077732374\n",
      "With N = 460000, omega = 0.04964277890917069\n",
      "With N = 480000, omega = 0.04867047569610589\n"
     ]
    }
   ],
   "source": [
    "Ns = [400000, 420000, 440000, 460000, 480000]\n",
    "dVC = 10\n",
    "delta = 0.05\n",
    "\n",
    "for N in Ns:\n",
    "    print(f'With N = {N}, omega = {original_vc_bound(N, dVC, delta, (2 * N) ** dVC)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closest numerical approximation of the sample size that the VC generalization bound predicts is 460000.\n",
    "\n",
    "**Question 1:** So the correct answer is [d] 460,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "With $N > d_{VC}$, we can use:\n",
    "$$m_{\\mathcal{H}}(N) = N^{d_{VC}}$$\n",
    "\n",
    "We also have $d_{VC} = 50$, $\\delta = 0.05$, and $N = 10000$.\n",
    "\n",
    "Therefore, the inequalities in sentence b, c, and d become:\n",
    "\n",
    "[b] Rademacher Penalty Bound: $\\epsilon \\leq \\sqrt{\\frac{2 \\ln(2N \\times N^{d_{VC}})}{N}} + \\sqrt{\\frac{2}{N} \\ln \\frac{1}{\\delta}} + \\frac{1}{N}$\n",
    "\n",
    "[c] Parrondo and Van den Broek: $\\epsilon \\leq \\sqrt{\\frac{1}{N} \\left( 2 \\epsilon + \\ln \\frac{6 \\times (2N)^{d_{VC}}}{\\delta} \\right)}$\n",
    "\n",
    "[d] Devroye: $\\epsilon \\leq \\sqrt{\\frac{1}{2N} \\left( 4\\epsilon (1 + \\epsilon) + \\ln \\frac{4 N^{2d_{VC}}}{\\delta} \\right)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rademacher_penalty_bound(N, dVC, delta, mH):\n",
    "    \"\"\"\n",
    "    Calculate the Rademacher penalty bound\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    N : int\n",
    "        Number of samples\n",
    "    dVC : int\n",
    "        VC dimension\n",
    "    delta : float\n",
    "        Confidence level\n",
    "    mH : int\n",
    "        The growth function\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    float\n",
    "        The Rademacher penalty bound\n",
    "    \"\"\"\n",
    "    return math.sqrt(2 * math.log(2 * N * mH) / N) + math.sqrt(2 * math.log(1 / delta) / N) + 1 / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parrondo_van_den_broek_bound(N, dVC, delta, mH):\n",
    "    \"\"\"\n",
    "    Calculate the Parrondo and Van den Broek bound\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    N : int\n",
    "        Number of samples\n",
    "    dVC : int\n",
    "        VC dimension\n",
    "    delta : float\n",
    "        Confidence level\n",
    "    mH : int\n",
    "        The growth function\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    float\n",
    "        The Parrondo and Van den Broek bound\n",
    "    \"\"\"\n",
    "    tmp = lambda eps: math.sqrt((2 * eps + math.log(6 * mH / delta)) / N) - eps\n",
    "    return optimize.brentq(tmp, 0, 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devroye(N, dVC, delta, mH):\n",
    "    \"\"\"\n",
    "    Calculate the Devroye bound\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    N : int\n",
    "        Number of samples\n",
    "    dVC : int\n",
    "        VC dimension\n",
    "    delta : float\n",
    "        Confidence level\n",
    "    mH : int\n",
    "        The growth function\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    float\n",
    "        The Devroye bound\n",
    "    \"\"\"\n",
    "    tmp = lambda eps: math.sqrt((4 * eps * (1 + eps) + math.log(4 / delta) + math.log(mH)) / (2 * N)) - eps\n",
    "    return optimize.brentq(tmp, 0, 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With N = 10000, dVC = 50, delta = 0.05, Original VC bound = 0.632174915200836\n",
      "With N = 10000, dVC = 50, delta = 0.05, Rademacher penalty bound = 0.3313087859616395\n",
      "With N = 10000, dVC = 50, delta = 0.05, Parrondo and Van den Broek bound = 0.22369829368078606\n",
      "With N = 10000, dVC = 50, delta = 0.05, Devroye bound = 0.21522804980824667\n"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "dVC = 50\n",
    "delta = 0.05\n",
    "\n",
    "print(f'With N = {N}, dVC = {dVC}, delta = {delta}, Original VC bound = {original_vc_bound(N, dVC, delta, (2 * N) ** dVC)}')\n",
    "print(f'With N = {N}, dVC = {dVC}, delta = {delta}, Rademacher penalty bound = {rademacher_penalty_bound(N, dVC, delta, N ** dVC)}')\n",
    "print(f'With N = {N}, dVC = {dVC}, delta = {delta}, Parrondo and Van den Broek bound = {parrondo_van_den_broek_bound(N, dVC, delta, (2 * N) ** dVC)}')\n",
    "print(f'With N = {N}, dVC = {dVC}, delta = {delta}, Devroye bound = {devroye(N, dVC, delta, N ** (dVC * 2))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At N = 10000, the Devroye bound is the smallest with $\\epsilon \\leq 0.215$\n",
    "\n",
    "**Question 2:** So the correct answer is [d] Devroye: $\\epsilon \\leq \\sqrt{\\frac{1}{2N} \\left( 4\\epsilon (1 + \\epsilon) + \\ln \\frac{4 m_{\\mathcal{H}}(N^2)}{\\delta} \\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "\n",
    "For the same value of $d_{VC}$ and $\\delta$ of problem 2, we will compute the bound for N = 5.\n",
    "\n",
    "Since $N < d_{VC}$, the approximation bound $N^{d_{VC}}$ cannot be used anymore. We will use $m_{\\mathcal{H}}(N) = 2^N$ instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With N = 5, dVC = 50, delta = 0.05, Original VC bound = 4.254597220000659\n",
      "With N = 5, dVC = 50, delta = 0.05, Rademacher penalty bound = 2.813654929686762\n",
      "With N = 5, dVC = 50, delta = 0.05, Parrondo and Van den Broek bound = 1.7439535969958095\n",
      "With N = 5, dVC = 50, delta = 0.05, Devroye bound = 2.264540762867992\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "dVC = 50\n",
    "delta = 0.05\n",
    "\n",
    "print(f'With N = {N}, dVC = {dVC}, delta = {delta}, Original VC bound = {original_vc_bound(N, dVC, delta, 2 ** (2 * N))}')\n",
    "print(f'With N = {N}, dVC = {dVC}, delta = {delta}, Rademacher penalty bound = {rademacher_penalty_bound(N, dVC, delta, 2 ** N)}')\n",
    "print(f'With N = {N}, dVC = {dVC}, delta = {delta}, Parrondo and Van den Broek bound = {parrondo_van_den_broek_bound(N, dVC, delta, 2 ** (2 * N))}')\n",
    "print(f'With N = {N}, dVC = {dVC}, delta = {delta}, Devroye bound = {devroye(N, dVC, delta, 2 ** (N ** 2))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At N = 5, the Parrondo and Van den Broek bound is the smallest with $\\epsilon \\leq 1.744$\n",
    "\n",
    "**Question 3:** So the correct answer is [c] Parrondo and Van den Broek: $\\epsilon \\leq \\sqrt{\\frac{1}{N} \\left( 2 \\epsilon + \\ln \\frac{6 m_{\\mathcal{H}}(2N)}{\\delta} \\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "This answer is referenced from [2].\n",
    "\n",
    "We have:\n",
    "\\begin{align}\n",
    "\\bar{g}(\\mathbf{x}) &\\approx \\frac{1}{K} \\sum_{k=1}^{K} g^{(\\mathcal{D}_k)}(\\mathbf{x}) \\\\\n",
    "&= \\frac{1}{K} \\sum_{k=1}^{K} a^{(\\mathcal{D}_i)} x \\\\\n",
    "&= x \\times \\frac{1}{K} \\sum_{k=1}^{K} a^{(\\mathcal{D}_i)} \\\\\n",
    "&= x\\hat{a} \\\\\n",
    "&= \\hat{a}x\n",
    "\\end{align}\n",
    "\n",
    "So the expected value $\\bar{g}(\\mathbf{x})$ is equal to $\\hat{a}x$.\n",
    "\n",
    "To find the expected value, we will do the experiments as follow: create two random points with x in range [-1, 1] and y calculated using the formula $f(x) = \\sin(\\pi x)$. Then we train the model using linear regression. We will do the experiment 1000 times and calculate the value $\\hat{a}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem4():\n",
    "    \"\"\"\n",
    "    Calculate the average of a\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    a_avg: float\n",
    "        The average of a\n",
    "    \"\"\"\n",
    "    N_runs = 1000   # number of runs\n",
    "    N = 2           # number of samples \n",
    "    a_total = 0     # total of a\n",
    "\n",
    "    # loop for RUNS times\n",
    "    for _ in range(N_runs):\n",
    "        # define two random points\n",
    "        x = np.random.uniform(-1, 1, N) # x1, x2\n",
    "        y = np.sin(np.pi * x)           # y1, y2\n",
    "\n",
    "        # train the model with linear regression\n",
    "        X = np.array([x]).T\n",
    "        w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "        a = w[0]\n",
    "\n",
    "        # add a to a_total\n",
    "        a_total += a\n",
    "\n",
    "    # calculate the average of a\n",
    "    a_avg = a_total / N_runs\n",
    "\n",
    "    return a_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average a = 1.44\n"
     ]
    }
   ],
   "source": [
    "# print round average a to 2 decimal digits\n",
    "print(f'Average a = {np.round(problem4(), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no answer match exactly to our result.\n",
    "\n",
    "**Question 4:** So the correct answer is [e] None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\n",
    "\n",
    "The bias is given by:\n",
    "$$\\text{bias}(x) = {(\\bar{g}(\\mathbf{x}) - f(x))}^2$$\n",
    "\n",
    "To find the bias, we will do the experiments as follow: create 1000 random test points with x in range [-1, 1] and y calculated using the formula $f(x) = \\sin(\\pi x)$. Then we train the model using linear regression for 1000 times and calculate the average square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias = 0.2966333276111952\n"
     ]
    }
   ],
   "source": [
    "N_runs_5 = 1000     # number of runs\n",
    "N_test_5 = 1000     # number of test points\n",
    "a_total_5 = 0       # total of a\n",
    "\n",
    "# generate test set\n",
    "x_test_5 = np.random.uniform(-1, 1, N_test_5)\n",
    "y_test_5 = np.sin(np.pi * x_test_5)\n",
    "\n",
    "# calculate the average of a\n",
    "a_avg_5 = problem4()\n",
    "\n",
    "# calculate predicted y\n",
    "y_pred_5 = a_avg_5 * x_test_5\n",
    "\n",
    "# calculate bias\n",
    "bias = np.mean((y_test_5 - y_pred_5) ** 2)\n",
    "\n",
    "# print bias\n",
    "print(f'Bias = {bias}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias calculated is closest to 0.3.\n",
    "\n",
    "**Question 5:** So the correct answer is [b] 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\n",
    "\n",
    "The variance is given by:\n",
    "$$\\text{var}(x) = {\\mathbb{E}}_{\\mathcal{D}}[{(g^{(\\mathcal{D})}(x) - \\bar{g}(\\mathbf{x}))}^2]$$\n",
    "\n",
    "To find the variance, we perform the following steps: create 1000 random test points x_test in the range [−1,1]. For each point, train the model 100 times using two random data points sampled from [−1,1]. For each run, calculate the squared difference between predictions using the model's slope and the average slope. Finally, average these squared differences across datasets and test points to compute the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance = 0.2389021765586611\n"
     ]
    }
   ],
   "source": [
    "# calculate average of a\n",
    "a_avg_6 = problem4()\n",
    "\n",
    "e_X = 0             # expectation over X\n",
    "N_runs_d_6 = 100    # number of runs over d\n",
    "N_runs_x_6 = 1000   # number of runs over x\n",
    "\n",
    "for _ in range(N_runs_x_6):\n",
    "    N = 2\n",
    "    x_test = np.random.uniform(-1, 1)\n",
    "    e_D = 0        # expectation over D\n",
    "\n",
    "    for _ in range(N_runs_d_6):\n",
    "        # define two random points\n",
    "        x = np.random.uniform(-1, 1, N)\n",
    "        y = np.sin(np.pi * x)\n",
    "\n",
    "        # train the model with linear regression\n",
    "        X = np.array([x]).T\n",
    "        w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "        a = w[0]\n",
    "\n",
    "        # calculate predicted y\n",
    "        y_pred = a * x_test\n",
    "\n",
    "        # calculate predicted y bar\n",
    "        y_pred_bar = a_avg_6 * x_test\n",
    "\n",
    "        # calculate expectation over D\n",
    "        e_D += (y_pred - y_pred_bar) ** 2 / N_runs_d_6\n",
    "\n",
    "    # calculate expectation over X\n",
    "    e_X += e_D / N_runs_x_6\n",
    "\n",
    "# calculate variance\n",
    "variance = e_X\n",
    "\n",
    "# print variance\n",
    "print(f'Variance = {variance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance calculated is closest to 0.2.\n",
    "\n",
    "**Question 6:** So the correct answer is [a] 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem7():\n",
    "    \"\"\"\n",
    "    Calculate the average of a for all the hypothesis\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    b_total_a: float\n",
    "        The total of b for the hypothesis h(x) = b\n",
    "    a_total_b: float\n",
    "        The total of a for the hypothesis h(x) = ax\n",
    "    a_total_c: float\n",
    "        The total of a for the hypothesis h(x) = ax + b\n",
    "    b_total_c: float\n",
    "        The total of b for the hypothesis h(x) = ax + b\n",
    "    a_total_d: float\n",
    "        The total of a for the hypothesis h(x) = ax^2\n",
    "    a_total_e: float\n",
    "        The total of a for the hypothesis h(x) = ax^2 + b\n",
    "    b_total_e: float\n",
    "        The total of b for the hypothesis h(x) = ax^2 + b\n",
    "    \"\"\"\n",
    "    N_runs = 1000   # number of runs\n",
    "    N = 2           # number of samples \n",
    "    b_total_a = 0   # total of b for the hypothesis h(x) = b\n",
    "    a_total_b = 0   # total of a for the hypothesis h(x) = ax\n",
    "    a_total_c = 0   # total of a for the hypothesis h(x) = ax + b\n",
    "    b_total_c = 0   # total of b for the hypothesis h(x) = ax + b\n",
    "    a_total_d = 0   # total of a for the hypothesis h(x) = ax^2\n",
    "    a_total_e = 0   # total of a for the hypothesis h(x) = ax^2 + b\n",
    "    b_total_e = 0   # total of b for the hypothesis h(x) = ax^2 + b\n",
    "\n",
    "    # loop for RUNS times\n",
    "    for _ in range(N_runs):\n",
    "        # define two random points\n",
    "        x = np.random.uniform(-1, 1, N) # x1, x2\n",
    "        y = np.sin(np.pi * x)           # y1, y2\n",
    "\n",
    "        # train the model with linear regression for h(x) = b\n",
    "        X_a = np.array([np.ones(N)]).T\n",
    "        w_a = np.dot(np.dot(np.linalg.inv(np.dot(X_a.T, X_a)), X_a.T), y)\n",
    "        b_a = w_a[0]\n",
    "\n",
    "        # train the model with linear regression for h(x) = ax\n",
    "        X_b = np.array([x]).T\n",
    "        w_b = np.dot(np.dot(np.linalg.inv(np.dot(X_b.T, X_b)), X_b.T), y)\n",
    "        a_b = w_b[0]\n",
    "\n",
    "        # train the model with linear regression for h(x) = ax + b\n",
    "        X_c = np.array([x, np.ones(N)]).T\n",
    "        w_c = np.dot(np.dot(np.linalg.inv(np.dot(X_c.T, X_c)), X_c.T), y)\n",
    "        a_c = w_c[0]\n",
    "        b_c = w_c[1]\n",
    "\n",
    "        # train the model with linear regression for h(x) = ax^2\n",
    "        X_d = np.array([x * x]).T\n",
    "        w_d = np.dot(np.dot(np.linalg.inv(np.dot(X_d.T, X_d)), X_d.T), y)\n",
    "        a_d = w_d[0]\n",
    "\n",
    "        # train the model with linear regression for h(x) = ax^2 + b\n",
    "        X_e = np.array([x * x, np.ones(N)]).T\n",
    "        w_e = np.dot(np.dot(np.linalg.inv(np.dot(X_e.T, X_e)), X_e.T), y)\n",
    "        a_e = w_e[0]\n",
    "        b_e = w_e[1]\n",
    "\n",
    "        # add total\n",
    "        b_total_a += b_a\n",
    "        a_total_b += a_b\n",
    "        a_total_c += a_c\n",
    "        b_total_c += b_c\n",
    "        a_total_d += a_d\n",
    "        a_total_e += a_e\n",
    "        b_total_e += b_e\n",
    "\n",
    "    # calculate the average \n",
    "    b_avg_a = b_total_a / N_runs\n",
    "    a_avg_b = a_total_b / N_runs\n",
    "    a_avg_c = a_total_c / N_runs\n",
    "    b_avg_c = b_total_c / N_runs\n",
    "    a_avg_d = a_total_d / N_runs\n",
    "    a_avg_e = a_total_e / N_runs\n",
    "    b_avg_e = b_total_e / N_runs\n",
    "\n",
    "    return b_avg_a, a_avg_b, a_avg_c, b_avg_c, a_avg_d, a_avg_e, b_avg_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute bias for all the hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias for h(x) = b: 0.507175366246416\n",
      "Bias for h(x) = ax: 0.26543395620941546\n",
      "Bias for h(x) = ax + b: 0.19381904552754087\n",
      "Bias for h(x) = ax^2: 0.5065178548334311\n",
      "Bias for h(x) = ax^2 + b: 3.7381660216148647\n"
     ]
    }
   ],
   "source": [
    "N_runs = 1000     # number of runs\n",
    "N_test = 1000     # number of test points\n",
    "b_total_a = 0   # total of b for the hypothesis h(x) = b\n",
    "a_total_b = 0   # total of a for the hypothesis h(x) = ax\n",
    "a_total_c = 0   # total of a for the hypothesis h(x) = ax + b\n",
    "b_total_c = 0   # total of b for the hypothesis h(x) = ax + b\n",
    "a_total_d = 0   # total of a for the hypothesis h(x) = ax^2\n",
    "a_total_e = 0   # total of a for the hypothesis h(x) = ax^2 + b\n",
    "b_total_e = 0   # total of b for the hypothesis h(x) = ax^2 + b\n",
    "\n",
    "# generate test set\n",
    "x_test = np.random.uniform(-1, 1, N_test)\n",
    "y_test = np.sin(np.pi * x_test)\n",
    "\n",
    "# calculate the average \n",
    "b_avg_a, a_avg_b, a_avg_c, b_avg_c, a_avg_d, a_avg_e, b_avg_e = problem7()\n",
    "\n",
    "# calculate predicted y\n",
    "y_pred_a = b_avg_a\n",
    "y_pred_b = a_avg_b * x_test\n",
    "y_pred_c = a_avg_c * x_test + b_avg_c\n",
    "y_pred_d = a_avg_d * x_test * x_test\n",
    "y_pred_e = a_avg_e * x_test * x_test + b_avg_e\n",
    "\n",
    "# calculate bias\n",
    "bias_a = np.mean((y_test - y_pred_a) ** 2)\n",
    "bias_b = np.mean((y_test - y_pred_b) ** 2)\n",
    "bias_c = np.mean((y_test - y_pred_c) ** 2)\n",
    "bias_d = np.mean((y_test - y_pred_d) ** 2)\n",
    "bias_e = np.mean((y_test - y_pred_e) ** 2)\n",
    "\n",
    "# print bias\n",
    "print(f'Bias for h(x) = b: {bias_a}')\n",
    "print(f'Bias for h(x) = ax: {bias_b}')\n",
    "print(f'Bias for h(x) = ax + b: {bias_c}')\n",
    "print(f'Bias for h(x) = ax^2: {bias_d}')\n",
    "print(f'Bias for h(x) = ax^2 + b: {bias_e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance for h(x) = b: 0.25033156750051694\n",
      "Variance for h(x) = ax: 0.23283189769471757\n",
      "Variance for h(x) = ax + b: 1.6622258546160749\n",
      "Variance for h(x) = ax^2: 16.480600412344224\n",
      "Variance for h(x) = ax^2 + b: 12739.209005568395\n"
     ]
    }
   ],
   "source": [
    "# calculate average\n",
    "b_avg_a, a_avg_b, a_avg_c, b_avg_c, a_avg_d, a_avg_e, b_avg_e = problem7()\n",
    "\n",
    "e_X_a, e_X_b, e_X_c, e_X_d, e_X_e = 0, 0, 0, 0, 0    # expectation over X for each hypothesis\n",
    "N_runs_d = 100    # number of runs over d\n",
    "N_runs_x = 1000   # number of runs over x\n",
    "\n",
    "for _ in range(N_runs_x):\n",
    "    N = 2\n",
    "    x_test = np.random.uniform(-1, 1)\n",
    "    e_D_a, e_D_b, e_D_c, e_D_d, e_D_e = 0, 0, 0, 0, 0  # expectation over D for each hypothesis\n",
    "\n",
    "    for _ in range(N_runs_d):\n",
    "        # define two random points\n",
    "        x = np.random.uniform(-1, 1, N)\n",
    "        y = np.sin(np.pi * x)\n",
    "\n",
    "        # train the model with linear regression for h(x) = b\n",
    "        X_a = np.array([np.ones(N)]).T\n",
    "        w_a = np.dot(np.dot(np.linalg.inv(np.dot(X_a.T, X_a)), X_a.T), y)\n",
    "        b_a = w_a[0]\n",
    "\n",
    "        # train the model with linear regression for h(x) = ax\n",
    "        X_b = np.array([x]).T\n",
    "        w_b = np.dot(np.dot(np.linalg.inv(np.dot(X_b.T, X_b)), X_b.T), y)\n",
    "        a_b = w_b[0]\n",
    "\n",
    "        # train the model with linear regression for h(x) = ax + b\n",
    "        X_c = np.array([x, np.ones(N)]).T\n",
    "        w_c = np.dot(np.dot(np.linalg.inv(np.dot(X_c.T, X_c)), X_c.T), y)\n",
    "        a_c = w_c[0]\n",
    "        b_c = w_c[1]\n",
    "\n",
    "        # train the model with linear regression for h(x) = ax^2\n",
    "        X_d = np.array([x * x]).T\n",
    "        w_d = np.dot(np.dot(np.linalg.inv(np.dot(X_d.T, X_d)), X_d.T), y)\n",
    "        a_d = w_d[0]\n",
    "\n",
    "        # train the model with linear regression for h(x) = ax^2 + b\n",
    "        X_e = np.array([x * x, np.ones(N)]).T\n",
    "        w_e = np.dot(np.dot(np.linalg.inv(np.dot(X_e.T, X_e)), X_e.T), y)\n",
    "        a_e = w_e[0]\n",
    "        b_e = w_e[1]\n",
    "        \n",
    "        # calculate predicted y\n",
    "        y_pred_a = b_a \n",
    "        y_pred_b = a_b * x_test\n",
    "        y_pred_c = a_c * x_test + b_c\n",
    "        y_pred_d = a_d * x_test * x_test\n",
    "        y_pred_e = a_e * x_test * x_test + b_e\n",
    "\n",
    "        # calculate predicted y bar\n",
    "        y_pred_bar_a = b_avg_a\n",
    "        y_pred_bar_b = a_avg_b * x_test\n",
    "        y_pred_bar_c = a_avg_c * x_test + b_avg_c\n",
    "        y_pred_bar_d = a_avg_d * x_test * x_test\n",
    "        y_pred_bar_e = a_avg_e * x_test * x_test + b_avg_e\n",
    "\n",
    "        # calculate expectation over D\n",
    "        e_D_a += (y_pred_a - y_pred_bar_a) ** 2 / N_runs_d\n",
    "        e_D_b += (y_pred_b - y_pred_bar_b) ** 2 / N_runs_d\n",
    "        e_D_c += (y_pred_c - y_pred_bar_c) ** 2 / N_runs_d\n",
    "        e_D_d += (y_pred_d - y_pred_bar_d) ** 2 / N_runs_d\n",
    "        e_D_e += (y_pred_e - y_pred_bar_e) ** 2 / N_runs_d\n",
    "\n",
    "    # calculate expectation over X\n",
    "    e_X_a += e_D_a / N_runs_x\n",
    "    e_X_b += e_D_b / N_runs_x\n",
    "    e_X_c += e_D_c / N_runs_x\n",
    "    e_X_d += e_D_d / N_runs_x\n",
    "    e_X_e += e_D_e / N_runs_x\n",
    "\n",
    "# calculate variance\n",
    "variance_a = e_X_a\n",
    "variance_b = e_X_b\n",
    "variance_c = e_X_c\n",
    "variance_d = e_X_d\n",
    "variance_e = e_X_e\n",
    "\n",
    "# print variance\n",
    "print(f'Variance for h(x) = b: {variance_a}')\n",
    "print(f'Variance for h(x) = ax: {variance_b}')\n",
    "print(f'Variance for h(x) = ax + b: {variance_c}')\n",
    "print(f'Variance for h(x) = ax^2: {variance_d}')\n",
    "print(f'Variance for h(x) = ax^2 + b: {variance_e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected out-of-sample error for h(x) = b: 0.7575069337469329\n",
      "Expected out-of-sample error for h(x) = ax: 0.498265853904133\n",
      "Expected out-of-sample error for h(x) = ax + b: 1.8560449001436157\n",
      "Expected out-of-sample error for h(x) = ax^2: 16.987118267177653\n",
      "Expected out-of-sample error for h(x) = ax^2 + b: 12742.947171590009\n"
     ]
    }
   ],
   "source": [
    "# print expected out-of-sample error for each hypothesis\n",
    "print(f'Expected out-of-sample error for h(x) = b: {bias_a + variance_a}')\n",
    "print(f'Expected out-of-sample error for h(x) = ax: {bias_b + variance_b}')\n",
    "print(f'Expected out-of-sample error for h(x) = ax + b: {bias_c + variance_c}')\n",
    "print(f'Expected out-of-sample error for h(x) = ax^2: {bias_d + variance_d}')\n",
    "print(f'Expected out-of-sample error for h(x) = ax^2 + b: {bias_e + variance_e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypothesis $h(x) = ax$ has the least expected value of out-of-sample error.\n",
    "\n",
    "**Question 7:** So the correct answer is [b] Hypotheses of the form $h(x) = ax$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VC Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "\n",
    "This answer is referenced from [3].\n",
    "\n",
    "According to the topic, we have $m_{\\mathcal{H}}(1) = 2$ and the growth function is given as:\n",
    "$$m_{\\mathcal{H}}(N + 1) = 2 m_{\\mathcal{H}}(N) - \\binom{N}{q}$$\n",
    "\n",
    "Suppose $N = d - 1$ the growth function becomes:\n",
    "\\begin{align}\n",
    "m_{\\mathcal{H}}(d - 1 + 1) &= 2 m_{\\mathcal{H}}(d - 1) - \\binom{d - 1}{q} \\\\\n",
    "\\Leftrightarrow m_{\\mathcal{H}}(d) &= 2 m_{\\mathcal{H}}(d - 1) - \\binom{d - 1}{q} \\\\\n",
    "\\end{align}\n",
    "\n",
    "The VC dimension is the largest value of N for which $m_{\\mathcal{H}}(N) = 2^N$ so the formula below becomes:\n",
    "\\begin{align}\n",
    "2^d &= 2 \\times 2^{d - 1} - \\binom{d - 1}{q} \\\\\n",
    "\\Leftrightarrow 2^d &= 2^d - \\binom{d - 1}{q} \\\\\n",
    "\\Leftrightarrow \\binom{d - 1}{q} &= 0\n",
    "\\end{align}\n",
    "\n",
    "But $\\binom{d - 1}{q}$ is only 0 when $q > d - 1$ or $d < q + 1$ which means $d \\leq q$. Since VC dimension is the largest value of N points that the hypothesis can shatter. $d = q$ is therefore the VC dimension of a hypothesis set whose growth function satisfies: $m_{\\mathcal{H}}(N + 1) = 2 m_{\\mathcal{H}}(N) - \\binom{N}{q}$\n",
    "\n",
    "**Question 8:** So the correct answer is [c] q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\n",
    "\n",
    "For lower bound:\n",
    "- Consider the intersection of hypothesis sets $\\bigcap_{k=1}^{K} H_k$. Among these, we will take $H_m$ as the one with smallest $d_{VC}$. The VC dimension of the intersection of hypothesis sets is limited by the hypothesis with the smallest $d_{VC}$. This is because the intersection cannot shatter more points than the set with the smallest VC dimension. So, the lower bound for $d_{VC}(\\bigcap_{k=1}^{K} H_k)$ is 0, as an empty set or singleton set has a VC dimension of 0.\n",
    "- Therefore the correct answer can only be [a] or [b] or [c].\n",
    "\n",
    "For upper bound:\n",
    "- Consider the intersection of hypothesis sets $\\bigcap_{k=1}^{K} H_k$. Among these, we will take $H_m$ as the one with smallest $d_{VC}$. When we find the VC dimension of the intersection of the sets, we can only use $H_m$ to shatter the maximum points. So, the VC dimension of $H_m$ is the upper bound for $d_{VC}(\\bigcap_{k=1}^{K} H_k)$\n",
    "\n",
    "**Question 9:** So the correct answer is [b] $0 \\leq d_{\\text{VC}}\\left( \\bigcap_{k=1}^{K} \\mathcal{H}_k \\right) \\leq \\min\\{ d_{\\text{VC}}(\\mathcal{H}_k) \\}_{k=1}^{K}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.\n",
    "This answer is referenced from [4].\n",
    "\n",
    "Assume that $H_1$ is a hypothesis where all points in the box [-1, 1] x [-1, 1] are classified as +1 and $H_2$ is a hypothesis set where all points are classified as -1.\n",
    "\n",
    "For $N = 1$, there are $2^N = 2^1 = 2$ possible dichotomies. However, $H_1$ cannot shatter all the points as it always outputs +1. $N = 1$ is therefore the break point of $H_1$ which means that the VC dimension of $H_1$ is 0. And $H_2$ is similar to $H_1$ as it always outputs -1, so the VC dimension of $H_2$ is 0 too.\n",
    "\n",
    "For $N = 1$, the union of $H_1$ and $H_2$ can shatter one point because it can produce 2 dichotomies -1 and +1. However for $N = 2$, it cannot shatter two points because it cannot generate all $2^2 = 4$ dichotomies. So we can infer that the VC dimension of $H_1 \\bigcup H_2$ is 1.\n",
    "\n",
    "For lower bound: \n",
    "- Consider the union of hypothesis sets $\\bigcup_{k=1}^{K} H_k$. Among these, we will take $H_m$ as the one with highest $d_{VC}$. When we find the VC dimension of the union of the sets, we can always use $H_m$ to shatter the maximum points. So, the VC dimension of $H_m$ must be at least a lower bound for $d_{VC}(\\bigcup_{k=1}^{K} H_k)$\n",
    "- Therefore the correct answer can only be [d] or [e].\n",
    "\n",
    "For upper bound:\n",
    "- Consider the upper bound in answer [d] with $K = 2$:\n",
    "$$\\sum_{k = 1}^{2} d_{VC}({\\mathcal{H}}_k) =  d_{VC}({\\mathcal{H}}_1) +  d_{VC}({\\mathcal{H}}_2) = 0 + 0 = 0$$\n",
    "$\\Rightarrow$ This doesn't match with the value found above.\n",
    "- Consider the upper bound in answer [e] with $K = 2$:\n",
    "$$2 - 1 + \\sum_{k = 1}^{2} d_{VC}({\\mathcal{H}}_k) = 1 + d_{VC}({\\mathcal{H}}_1) +  d_{VC}({\\mathcal{H}}_2) = 1 + 0 + 0 = 1$$\n",
    "$\\Rightarrow$ This matches with the value found above.\n",
    "\n",
    "**Question 10:** So the correct answer is [e] $\\max\\{ d_{\\text{VC}}(\\mathcal{H}_k) \\}_{k=1}^{K} \\leq d_{\\text{VC}}\\left( \\bigcup_{k=1}^{K} \\mathcal{H}_k \\right) \\leq K - 1 + \\sum_{k=1}^{K} d_{\\text{VC}}(\\mathcal{H}_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1]: homefish. \"edX Learning From Data 2017\" <i>Github</i>, 2017, https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_4/homework_4_problem_1_Generalization_Error.ipynb. (Accessed date: 01/12/2024)\n",
    "\n",
    "[2]: homefish. \"edX Learning From Data 2017\" <i>Github</i>, 2017, https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_4/homework_4_problem_4_5_6_Bias_and_Variance.ipynb. (Accessed date: 01/12/2024)\n",
    "\n",
    "[3]: homefish. \"edX Learning From Data 2017\" <i>Github</i>, 2017, https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_4/homework_4_problem_8_VC_dimension.ipynb. (Accessed date: 01/12/2024)\n",
    "\n",
    "[4]: homefish. \"edX Learning From Data 2017\" <i>Github</i>, 2017, https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_4/homework_4_problem_9_10_VC_dimension.ipynb. (Accessed date: 01/12/2024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
